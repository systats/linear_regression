[
["lm-residuenanalyse.html", "4 LM Residuenanalyse 4.1 Linearität der Parameter 4.2 Unabhängigkeit der Residuen 4.3 Homoskedastizität 4.4 Normalverteilung der Residuen 4.5 Multikolinearität 4.6 Ausreißer 4.7 Beipiel References", " 4 LM Residuenanalyse load(url(&#39;https://github.com/systats/macro_project/raw/master/project_macro.Rdata&#39;)) # filter data dat &lt;- project_macro %&gt;% filter(year == 2010) fit0 &lt;- lm(gov_cens ~ 1, data = dat) # Nullmodell fit1 &lt;- lm(gov_cens ~ engage_soc, data = dat) # bivariat fit2 &lt;- lm(gov_cens ~ engage_soc + pol_stability, data = dat) # multivariat Eine Residuenanalyse dient dazu Modellannahmen statistischer Methoden bezüglich der Verteilung der Daten zu überprüfen. Damit soll die Validität und Reliabilität der Ergebnisse sichergestellt werden. Wenn alle Annahmen erfüllt sind, ist die Parameterschätzung effizient unter allen linearen, unverzerrten Schätzern was auch Best Linear Unbiased Estimator (BLUE, Gauß-Markov-Theorem) genannt wird (Urban and Mayerl 2011: 120). Wenn hingegen eine eindeutige Annahmenverletzung vorliegt sind die Parameter verzerrt und können nicht interpretiert werden. Zur Prüfung der Modellannahmen werden neben formalen Tests verstärkt Visualisierungen genutzt. So werden Modellierungsprobleme entdeckt und verbessert. Die Chancen der Modell-Verbesserung wahrzunehmen, entspricht der Grundhaltung der explorativen Datenanalyse ethz statistics. Es geht hier nicht um präzise mathematische Aussagen, Optimalität von statistischen Verfahren oder um Signifikanz, sondern um Methoden zum kreativen Entwickeln von Modellen, die die Daten gut beschreiben. Je nach Lektüre werden 3 - 10 Modellannahmen angegeben, wobei manchmal auch Multikolinearität und Ausreißer dazu gezählt werden, welche hier separat behandelt werden. Die folgenden vier Annahmen sollten getestet werden: Linearität der Parameter Unabhängigkeit der Residuen Homoskedastizität Normalverteilung der Residuen: Alle benötigten Informationen können mit augment vom broom package extrahiert werden. results &lt;- augment(fit2, dat) # results %&gt;% head 4.1 Linearität der Parameter The most important mathematical assumption of LM is that its deterministic components is a linear function of the seperate predictors: \\(y = \\beta_1 x_1 + \\beta_2 x_2 + ...\\) (Gelman and Hill 2007: 45). Die Lineare Regression schätzt die \\(\\beta\\) Parameter intrinsisch linear (Urban and Mayerl 2011: 207). Nicht lineare Assoziationen sind ohne zusätzliche Spezifikationen nicht erfassbar. Allerdings wird jeder Parameter separat (partiell) geschätzt (Additivität), wodurch die x-Variablen nicht-linear transformiert und trotzdem die Parameter linear interpretiert werden können (Urban and Mayerl 2011: 211). ggplot(results, aes(engage_soc, gov_cens)) + # aes(x, y) geom_point() + geom_smooth(method = &quot;lm&quot;, se = F) + geom_smooth(method = &quot;loess&quot;, se = F, color = &quot;red&quot;) ggplot(results, aes(pol_stability, gov_cens)) + # aes(x, y) geom_point() + geom_smooth(method = &quot;lm&quot;, se = F) + geom_smooth(method = &quot;loess&quot;, se = F, color = &quot;red&quot;) Die blaue Linie zeigt den linearen Zusammenhang zwischen x und y. Durch method = &quot;lm&quot; kann das einfach dargestellt werden. Die Steigungen der gezeigten Regressionsgeraden entsprechen den geschätzten Parametern. Zusätzlich kann durch method = &quot;loess&quot; (rot) eine robuste Glättungsmethode eingesetzt werden, die auch nicht-lineare Tendenzen identifiziert und Ausreißer ignoriert. Das Streudiagramm für engage_soc und gov_cens zeigt einen eindeutig positiv, linearen Zusammenhang an. Zwar weißt die loess Funktion des rechten Streudiagramms für pol_stability auf eine nicht-linearen Zusammenhang hin, doch immerhin stellt die Regressionsgerade eine gute Approximation dar. 4.2 Unabhängigkeit der Residuen Sind die unbeobachteten Fehler \\(\\varepsilon_i\\) unabhängig und unkorreliert zu x geschätzt (Gelman and Hill 2007: 46)? Zwar ist der bedingte Erwartungswert zwischen Prädikatoren und den Residuen per (OLS) Definition Null, dennoch sollte diese Annahme getestet werden, da durch Spezifikationsfehler z.B. Autokorrelation1 oder omitted variable bias die Standardfehler falsch geschätzt werden könnten. \\[cov(x_i,\\varepsilon_i) = 0\\] Um die Unabhängigkeit der Residuen zu testen, wird für jede Variable separat ein eigens Streudiagramm erstellt. Diese werden dann nach auffälligen Datenmustern (Korrelationen) untersucht. Wieder kommt neben lm auch loess zum Einsatz. ggplot(data = results, aes(x = engage_soc, y = .resid)) + geom_point() + geom_smooth(method = &quot;lm&quot;, se = F) + geom_smooth(method = &quot;loess&quot;, se = F, color = &quot;red&quot;) ggplot(data = results, aes(x = pol_stability, y = .resid)) + geom_point() + geom_smooth(method = &quot;lm&quot;, se = F) + geom_smooth(method = &quot;loess&quot;, se = F, color = &quot;red&quot;) Die blaue Linie symbolisiert die Regressionsgerade und darum streuen die unstandardisierten Residuen. Die Kernstreuung der Residuen (exklusive Ausreißer) scheint für beide Variablen unkorreliert zu sein. Neben einigen wenigen Ausreißern scheint keine der Variablen eine systematische Korrelation zu den Residuen zu besitzen. 4.3 Homoskedastizität Die Fehlervarianz \\(\\varepsilon_i\\) ist homoskedatisch, wenn die Residuen zufällig, gleichverteilt um \\(\\hat y\\) streuen (Gelman and Hill 2007: 46). Varianzhomogenität besagt, dass die Fehlerstreuung über alle Beobachtungen hinweg identisch verteilt ist (Urban and Mayerl 2011: 242). Damit wird jedem Fehler das gleich Gewicht zugeschrieben \\(\\varepsilon_i = (y_i-\\hat y)^2\\). \\[var(\\varepsilon_i)=\\sigma^2\\] Wenn diese Annahme verletzt ist spricht man von Heteroskedatiszität, wodurch zwar die \\(\\beta\\) Parameter unverzerrt geschätzt werden, allerdings die Standardfehler \\(SE(\\beta_1)\\) unter- oder überschätzt (under-, overestimated) werden (Urban and Mayerl 2011: 243). Unterschätzung der Standardfehler führt zu erhöhtem Type I Error \\(\\alpha\\) (also \\(H_0\\) wird zu häufig verworfen) und eine Unterschätzung reduziert die statistische Power (weniger Teststärke führt zu: \\(H_0\\) wird zu häufig angenommen). Während heteroskedastischen Residuen die Parameter nicht beeinflussen, können die Signifikanztest durch die Verzerrung nicht mehr interpretiert werden. Lösungen: die y-Variable transformiert werden neue Variablen ins Modell aufnehmen (omitted variables, Dummies erstellen für jede Varianzgruppe) Robust Huber-White Sandwiche estimator um ungleiche Varianzen zu schätzen (see package ‘sandwich’ Lumley and Zeileis 2015) oder geeignete Panel oder Multilevel Modelle (Gelman and Hill 2007). results %&gt;% ggplot(aes(.fitted, .std.resid)) + geom_point() + geom_smooth(se = F, color = &quot;red&quot;) + geom_hline(yintercept = 0, color = &quot;blue&quot;) + geom_hline(aes(yintercept = - sd(.std.resid)), linetype = 2) + geom_hline(aes(yintercept = sd(.std.resid)), linetype = 2) + ggtitle(&quot;Standardized Residuals vs Fitted&quot;) ggplot(results, aes(.fitted, sqrt(.std.resid))) + geom_jitter() + geom_smooth(se = FALSE, color = &quot;red&quot;) + ggtitle(&quot;Scale-Location&quot;) Auf der x-Achse wurden die geschätzten Werte (\\(\\hat y_i = \\beta_0 + \\beta_1x_i\\)) und auf der y-Achse die unstandardisierten Residuen abgetragen. Die zwei gestrichelten, horizontalen Linien umfassen 68,3% der Fälle und unterstützen dabei die Kernstreuung visuell zu identifizieren. Abgesehen von ein paar wenigen Ausreißern streuen die Residuen gleichverteilt um die Vorhersage. Zur abschließenden Klarheit kann ein Levene-Test durchgeführt werden (Urban and Mayerl 2011: 248), dessen Nullhypothese lautet H0: die Varianzunterschiede zwischen den betrachteten Gruppen sind gleich Null. Zur Durchführung des Levine-Tests muss eine Gruppenvariable erstellt werden, welche die visuell inspizierten Varianzgruppen in einem Vektor repräsentieren. Wird der Test signifikant (\\(p&lt;0.05^*\\)) ist die Annahme verletzt und man sollte zu Methoden zur robusten Schätzung der Standardfehler (SE) greifen. 4.3.1 Gewichtete lineare Regression Die Varianzen der einzelnen Zufallsfehler, die wir mit \\(\\sigma_i^2 = \\sum \\varepsilon_i^2\\) bezeichnen wollen, sollen nun nicht mehr als gleich \\(\\sigma^2\\) vorausgesetzt werden. Dann ist es sicher sinnvoll, den Beobachtungen mit kleinerer Zufallsstreuung, also den präziseren Beobachtungen, in der Regressionsrechnung grösseres Gewicht zu geben. Statt der gewöhnlichen Quadratsumme SSQ(E) kann man eine gewichtete Version davon, \\(\\sum w_iR^2_i\\), minimieren. Die Gewichte \\(w_i\\) sollen für steigende \\(\\sigma_i\\) fallen. Nach dem Prinzip der Maximalen Likelihood ist \\(w_i = 1/\\sigma^2_i\\) optimal. 4.4 Normalverteilung der Residuen Die Normalverteilung der Residuen is the generally the least important assumption (Gelman and Hill 2007: 46). Die Schätzung der \\(\\beta\\) Parameter ist davon überhaupt nicht beeinflusst. Die Signifikanz welche auf asymptotisch, normal verteilten Zufallsvariablen beruht hingegen schon (Urban and Mayerl 2011: 193). \\[\\varepsilon_i \\sim N(0, \\sigma^2)\\] Sollte ein Modell alle Annahmen erfüllen, spricht man von independent and identically distributed errors (iid). Anders formuliert - konsistente, erwartungstreue und normal verteilte Fehler. \\[\\varepsilon_i \\stackrel{iid}{\\sim} N(0, \\sigma^2)\\] ggplot(data = results, aes(.std.resid)) + geom_histogram(aes(y = ..density..)) + stat_function(fun = dnorm, lwd = 1, col = &#39;red&#39;) + ggtitle(&quot;Standardnormalverteilung der Residuen&quot;) ggplot(data = results, aes(sample = .std.resid)) + stat_qq() + geom_abline(colour = &quot;red&quot;) + ggtitle(&quot;Q-Q Plot&quot;) Das kann mit einer einfachen Häufigkeitsverteilung der Residuen visuell eingeschätzt werden. Folgt die empirische Verteilung (schwarz) der theoretischen Normalverteilung (rot)? Die Residuen streuen annähernd normal verteilt um 0 mit einer konstanten Varianz. Beim Q-Q-Plot sollten die Punkte so nahe wie möglich an der roten Linie liegen. Abgesehen von ein paar Ausreißern am Ende der Verteilung sind die annähernd Residuen normal verteilt. 4.5 Multikolinearität (Multi-) Kollinearität beschreibt wie stark zwei (oder mehr) unabhängige x-Variablen linear voneinander abhängig sind (Urban and Mayerl 2011: 225). Mit steigender Multikolinearität werden die Standardfehler einer Regression überschätzt und die wahren Effekte jeder einzelnen Variablen können nicht mehr identifiziert werden. Zwei kollineare Variablen besitzen einen gemeinsamen Informationsanteil, der nicht durch das statistische Modell separat geschätzt werden kann. Zum Beispiel: leg height example. Die Kolinearität zweier Variablen kann beschrieben werden durch \\[x_{i1} = \\beta_0 + \\beta_1x_{i2}\\] Mit dem Toleranz-Test kann eingeschätzt werden, wie hoch der eigene Erklärungsbeitrag einer X- Variablen nach Kontrolle der übrigen Variablen ist (c.p) (Urban and Mayerl 2011: 231). \\[T_p = 1−R_p^2\\] Ein Wert von kleiner 0.2 deutet auf eine starke Multikollinearität hin (lediglich ein Fünftel eigene Varianz). Werte nahe 0 implizieren, dass die jeweilige \\(x_k\\)-Variable nur noch sehr wenig eigene Erklärungsanteile erbringt bzw. umgekehrt. Liegen viele niedrige Toleranzwerte vor, sollten Stabilitätstests durchgeführt werden. Häufiger jedoch wird der Variance Inflation Factor berechnet, der auf die Logik der Toleranz zurückgreift, allerdings eine bessere Interpretation bietet. \\[VIF_k= \\frac{1}{T_p} = \\frac{1}{(1-R_p^2)}\\] Je größer der VI-Faktor einer Variablen k, desto stärker sind die Hinweise auf Multikollinearität. Als Daumenregel werden häufig VIF-Werte von über 10 als zu hoch eingestuft. Andere bezeichnen \\(VIF_p &gt; 5\\) zu hoch, da die die Erklärungsleistung einer Variable \\(\\rightarrow 1/0.2\\) unter 20% fällt (Urban and Mayerl 2011: 232). library(car) car::vif(fit2) ## engage_soc pol_stability ## 1.144117 1.144117 vif_data &lt;- tidy(vif(fit2)) vif_data %&gt;% ggplot(aes(names, x)) + geom_bar(stat = &quot;identity&quot;, width = 0.5) + geom_hline(yintercept = 5, linetype = 2, alpha = 0.7) + geom_hline(yintercept = 10, linetype = 2, alpha = 0.7) + annotate(&quot;text&quot;, x = 1, y = 4.7, label = &quot;gut&quot;) + annotate(&quot;text&quot;, x = 1, y = 9.7, label = &quot;akzeptabel&quot;) + ggtitle(&quot;Variance Inflation Factors (multicollinearity)&quot;) Die VIF-Werte werden wie das \\(R^2\\) direkt durch den Stichprobenumfang und die Stichprobenvarianz beeinflusst (O’brien 2007). Außerdem steigt natürlich mit der Anzahl der Prädiktoren auch die Wahrscheinlichkeit Multikolinearität zu erhalten. Wenn hohe VIF Werte vorliegen stehen mindestens diese Optionen zur Verfügung: entferne die problematische(n) Variable(n). komprimiere hoch korrelierten Variablen in eine Dimension (additiven/multiplikativer Index, PCA, EFA) 4.6 Ausreißer OLS minimiert die quadratischen Residuen \\(\\sum \\varepsilon_i^2\\). Dadurch können Ausreißer die Regressionsgerade sensible beeinflussen, was nach Ausschluss der extremen Werte zu signifikant verschiedenen Ergebnissen führen kann. Ausreißer können Messfehler oder einfach nur extreme Ausprägungen darstellen. Darum sollte transparent dokumentiert werden, wie stabil die Ergebnisse vor und nach dem Ausschluss sind. results %&gt;% arrange(.cooksd) %&gt;% select(country, engage_soc, pol_stability, gov_cens, .fitted, .resid) %&gt;% top_n(5) ## country engage_soc pol_stability gov_cens .fitted .resid ## 1 Cabo Verde 1.3365581 0.8571441 3.2131814 1.6164876 1.596694 ## 2 Cabo Verde 1.3365581 0.8571441 3.2131814 1.6164876 1.596694 ## 3 Solomon Islands 0.1275224 0.4108825 2.2074433 0.4031334 1.804310 ## 4 Belgium 1.3411289 0.7815902 3.7380989 1.5924303 2.145669 ## 5 Somalia -1.3773521 -3.1063728 0.4572907 -2.2054820 2.662773 # Points size reflecting Cook&#39;s distance ggplot(data = results, aes(x = .fitted, y = .resid, size = .cooksd)) + geom_hline(yintercept = 0, colour = &quot;firebrick3&quot;) + geom_point(alpha = .5) + geom_text(aes(label = rownames(results))) + scale_size_area(&quot;Cook’s distance&quot;) jacknife &amp; bootstrap: both test and solution 4.7 Beipiel Die oben einzeln vorgestellten Konzepte zur Residuenanalyse werden beispielhaft mit dem sjPlot package durchgeführt. Mit dem Argument type = &quot;ma&quot; für model assessment können alle Modellannahmen getestet werden. ACHTUNG: Die Anforderungen in deinem Kurs können von dieser praktischen Vorgehensweise abweichen. library(sjPlot) fit2_ma &lt;- sjp.lm(fit2, type = &quot;ma&quot;) ## # A tibble: 2 x 3 ## models adjusted.r2 aic ## &lt;chr&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 original 0.6434510 432.8847 ## 2 updated 0.6804059 410.5064 Die erste Tabelle zeigt die Modellgüte für das ursprüngliche Model (original) in Form von \\(R^2\\) and AIC. Darunter wird die Modellgüte für das aktualisierte Modell (updated) berichtet, welches automatisch die folgenden Ausreißer identifiziert, ausgeschlossen und das Regressionsmodel neu berechnet hat: fit2_ma$outlier ## [1] 138 146 Die Unterschiede sind verschwindend gering, dennoch werden die Ausreißer ausgeschlossen (nicht notwendig) no_outliers &lt;- dat[ - fit2_ma$outlier, ] # Zwei Ausreißer entfernt Als nächstes wird das statistische Modell auf Multikolinearität untersucht. Der Variance Inflation Factor für beide x-Variablen ist weit unter der geforderten Grenzwerten. Das statistische Modell hat damit keine Probleme mit zu hoch korrelierten Variablen. Nun werden die BLUE Annahmen getestet. Die Residuen folgen der theoretischen Normalverteilung (QQ-Plot) und nur die angegeben Ausreißer weichen signifikant davon ab. Auch das Histogram unterstützt die Annahme von normal verteilten Fehlern \\(\\varepsilon_i \\sim N(0, \\sigma^2)\\). Zuletzt wird die Streuung der Residuen betrachtet. Die Residuen streuen homoskedastisch um die Vorhersage und keine Muster sind identifizierbar. Damit sind die Annahmen \\(\\sum \\varepsilon^2 = \\sigma^2\\) und \\(cov(x_i, \\varepsilon_i) = 0\\) erfüllt und die Parameter, sowie Signifikanztest können interpretiert werden. References "]
]
